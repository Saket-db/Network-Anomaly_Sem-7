{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dCBDn65hcjla"
      },
      "outputs": [],
      "source": [
        "# Notebook: contrastive_tabular_simclr.ipynb\n",
        "# Requirements: torch, torchvision, sklearn, pandas, numpy, tqdm, matplotlib\n",
        "\n",
        "import os, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, roc_auc_score, accuracy_score\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "H3TT_bvQqU6K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a4b2404-5abe-49dc-ea17-865941aaf2b0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create folder\n",
        "!mkdir -p /content/data/nsl_kdd\n",
        "\n",
        "# Download NSL-KDD train/test text files\n",
        "!wget -q -O /content/data/nsl_kdd/KDDTrain+.txt https://raw.githubusercontent.com/defcom17/NSL_KDD/master/KDDTrain+.txt\n",
        "!wget -q -O /content/data/nsl_kdd/KDDTest+.txt  https://raw.githubusercontent.com/defcom17/NSL_KDD/master/KDDTest+.txt"
      ],
      "metadata": {
        "id": "SZVxBdc8gXzl"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load dataset (same assumption as Notebook 1)\n",
        "DATA_PATH = \"/content/data/nsl_kdd/KDDTrain+.txt\"  # replace\n",
        "# Load the dataset without a header\n",
        "df = pd.read_csv(DATA_PATH, header=None)\n",
        "# The second to last column is the attack type label (index -2)\n",
        "label_col_index = df.columns[-2]\n",
        "\n",
        "# Print unique values in the original label column to help diagnose\n",
        "print(\"Unique original labels:\", df[label_col_index].unique())\n",
        "\n",
        "# Map 'normal' and 'normal.' to 0, and all other labels to 1\n",
        "# Strip whitespace and convert to lower case for more robust matching\n",
        "df['binary_label'] = df[label_col_index].apply(lambda x: 0 if str(x).strip().lower() in ['normal', 'normal.'] else 1)\n",
        "\n",
        "# Print the count of each binary label to confirm both classes are present\n",
        "print(\"Binary label counts:\\n\", df['binary_label'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDKNZMJEftBf",
        "outputId": "d2d3fd66-7780-4386-f4fb-216d9c525bbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique original labels: ['normal' 'neptune' 'warezclient' 'ipsweep' 'portsweep' 'teardrop' 'nmap'\n",
            " 'satan' 'smurf' 'pod' 'back' 'guess_passwd' 'ftp_write' 'multihop'\n",
            " 'rootkit' 'buffer_overflow' 'imap' 'warezmaster' 'phf' 'land'\n",
            " 'loadmodule' 'spy' 'perl']\n",
            "Binary label counts:\n",
            " binary_label\n",
            "0    67343\n",
            "1    58630\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Preprocess (scale numerics, one-hot category)\n",
        "# Explicitly define known categorical columns for NSL-KDD\n",
        "cat_cols = [1, 2, 3] # protocol_type, service, flag\n",
        "\n",
        "# Assuming all other feature columns (0-37 excluding the label and the explicit cat cols) are numeric\n",
        "all_features_indices = list(range(38))\n",
        "num_cols = [col for col in all_features_indices if col not in cat_cols]\n",
        "\n",
        "# Get the actual column names from the indices - this step is not needed if we use indices directly in ColumnTransformer\n",
        "# num_col_names = df.columns[num_cols].tolist()\n",
        "# cat_col_names = df.columns[cat_cols].tolist()\n",
        "\n",
        "transformer = ColumnTransformer([\n",
        "    (\"num\", StandardScaler(), num_cols),\n",
        "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols)\n",
        "], remainder='drop')\n",
        "\n",
        "# Select only the feature columns (0-37) for transformation\n",
        "X_all = transformer.fit_transform(df[all_features_indices])\n",
        "y_all = df['binary_label'].values\n",
        "print(\"Feature dim:\", X_all.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOZUeGLSgK7r",
        "outputId": "9c60133d-cd25-4111-f058-450e85a74345"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature dim: (125973, 119)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Create PyTorch Dataset with simple augmentations for contrastive learning\n",
        "class TabularContrastiveDataset(Dataset):\n",
        "    def __init__(self, X):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.n = self.X.shape[0]\n",
        "    def __len__(self):\n",
        "        return self.n\n",
        "    def augment(self, x):\n",
        "        # Two simple augmentations: gaussian noise and feature dropout (mask)\n",
        "        x = x.clone()\n",
        "        # gaussian noise\n",
        "        noise = torch.randn_like(x) * 0.01\n",
        "        x = x + noise\n",
        "        # random feature mask\n",
        "        mask = (torch.rand_like(x) > 0.1).float()  # 10% drop\n",
        "        x = x * mask\n",
        "        return x\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.X[idx]\n",
        "        xi = self.augment(x)\n",
        "        xj = self.augment(x)\n",
        "        return xi, xj"
      ],
      "metadata": {
        "id": "Ss97uH3kgfsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Tiny MLP encoder and projection head\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=256, proj_dim=64):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.projector = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, proj_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(proj_dim, proj_dim)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        h = self.net(x)\n",
        "        z = self.projector(h)\n",
        "        return h, z"
      ],
      "metadata": {
        "id": "Kd3ckRJlgqSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. NT-Xent contrastive loss implementation\n",
        "def nt_xent_loss(z_i, z_j, temperature=0.5):\n",
        "    # z_i, z_j: (batch_size, proj_dim)\n",
        "    batch_size = z_i.shape[0]\n",
        "    z = torch.cat([z_i, z_j], dim=0)  # 2N x d\n",
        "    z = nn.functional.normalize(z, dim=1)\n",
        "    sim = torch.matmul(z, z.T)  # 2N x 2N\n",
        "    # create mask to remove similarity with itself\n",
        "    mask = (~torch.eye(2*batch_size, dtype=bool)).to(z.device)\n",
        "    positives = torch.cat([torch.diag(sim, batch_size), torch.diag(sim, -batch_size)], dim=0)\n",
        "    nom = torch.exp(positives / temperature)\n",
        "    denom = torch.sum(torch.exp(sim / temperature) * mask.float(), dim=1)\n",
        "    loss = -torch.log(nom / denom)\n",
        "    return loss.mean()"
      ],
      "metadata": {
        "id": "9BkwOoxLgtfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Prepare dataset loaders (use only training split for SSL)\n",
        "X_train_ssl, X_rest, y_train_ssl, y_rest = train_test_split(X_all, y_all, test_size=0.5, random_state=42, stratify=y_all)\n",
        "\n",
        "# Check if X_train_ssl is a sparse matrix and convert to dense if necessary\n",
        "if hasattr(X_train_ssl, 'toarray'):\n",
        "    X_train_ssl_dense = X_train_ssl.toarray()\n",
        "else:\n",
        "    X_train_ssl_dense = X_train_ssl\n",
        "\n",
        "ssl_dataset = TabularContrastiveDataset(X_train_ssl_dense)\n",
        "\n",
        "ssl_loader = DataLoader(ssl_dataset, batch_size=256, shuffle=True, drop_last=True)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "encoder = Encoder(input_dim=X_all.shape[1], hidden_dim=256, proj_dim=64).to(device)\n",
        "optimizer = optim.Adam(encoder.parameters(), lr=1e-3, weight_decay=1e-6)"
      ],
      "metadata": {
        "id": "zJM6KoSKgwBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Train encoder with contrastive loss\n",
        "epochs = 30\n",
        "for epoch in range(epochs):\n",
        "    encoder.train()\n",
        "    total_loss = 0.0\n",
        "    for xi, xj in ssl_loader:\n",
        "        xi = xi.to(device); xj = xj.to(device)\n",
        "        _, zi = encoder(xi)\n",
        "        _, zj = encoder(xj)\n",
        "        loss = nt_xent_loss(zi, zj, temperature=0.5)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    avg = total_loss / len(ssl_loader)\n",
        "    print(f\"Epoch {epoch+1}/{epochs} - contrastive loss: {avg:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBc4kqaFg0ou",
        "outputId": "741420c2-e422-48bd-d28c-96e5f724f678"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30 - contrastive loss: 4.6214\n",
            "Epoch 2/30 - contrastive loss: 4.5303\n",
            "Epoch 3/30 - contrastive loss: 4.5100\n",
            "Epoch 4/30 - contrastive loss: 4.4958\n",
            "Epoch 5/30 - contrastive loss: 4.4890\n",
            "Epoch 6/30 - contrastive loss: 4.4781\n",
            "Epoch 7/30 - contrastive loss: 4.4728\n",
            "Epoch 8/30 - contrastive loss: 4.4690\n",
            "Epoch 9/30 - contrastive loss: 4.4650\n",
            "Epoch 10/30 - contrastive loss: 4.4635\n",
            "Epoch 11/30 - contrastive loss: 4.4615\n",
            "Epoch 12/30 - contrastive loss: 4.4549\n",
            "Epoch 13/30 - contrastive loss: 4.4543\n",
            "Epoch 14/30 - contrastive loss: 4.4509\n",
            "Epoch 15/30 - contrastive loss: 4.4490\n",
            "Epoch 16/30 - contrastive loss: 4.4479\n",
            "Epoch 17/30 - contrastive loss: 4.4425\n",
            "Epoch 18/30 - contrastive loss: 4.4423\n",
            "Epoch 19/30 - contrastive loss: 4.4390\n",
            "Epoch 20/30 - contrastive loss: 4.4360\n",
            "Epoch 21/30 - contrastive loss: 4.4357\n",
            "Epoch 22/30 - contrastive loss: 4.4333\n",
            "Epoch 23/30 - contrastive loss: 4.4309\n",
            "Epoch 24/30 - contrastive loss: 4.4289\n",
            "Epoch 25/30 - contrastive loss: 4.4287\n",
            "Epoch 26/30 - contrastive loss: 4.4271\n",
            "Epoch 27/30 - contrastive loss: 4.4254\n",
            "Epoch 28/30 - contrastive loss: 4.4256\n",
            "Epoch 29/30 - contrastive loss: 4.4244\n",
            "Epoch 30/30 - contrastive loss: 4.4214\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Build representation for downstream classification (freeze encoder)\n",
        "encoder.eval()\n",
        "with torch.no_grad():\n",
        "    X_repr_list = [] # Use a different name to avoid confusion with the final X_repr\n",
        "    batch_size = 512\n",
        "    # Check if X_all is sparse\n",
        "    is_sparse = hasattr(X_all, 'todense')\n",
        "\n",
        "    for i in range(0, X_all.shape[0], batch_size):\n",
        "        xb_slice = X_all[i:i+batch_size]\n",
        "        if is_sparse:\n",
        "            # Convert the sparse slice to a dense NumPy array before creating the tensor\n",
        "            xb_dense = xb_slice.todense()\n",
        "        else:\n",
        "            # If X_all is already dense, use the slice directly\n",
        "            xb_dense = xb_slice\n",
        "\n",
        "        xb = torch.tensor(xb_dense, dtype=torch.float32).to(device)\n",
        "        h, z = encoder(xb)\n",
        "        # use normalized z as representation\n",
        "        X_repr_list.append(nn.functional.normalize(z, dim=1).cpu().numpy())\n",
        "\n",
        "# Stack the list of arrays into a single NumPy array\n",
        "if X_repr_list: # Check if the list is not empty\n",
        "    X_repr = np.vstack(X_repr_list)\n",
        "else:\n",
        "    X_repr = np.array([]) # Create an empty numpy array if the list is empty\n",
        "    print(\"Warning: X_repr_list is empty. No representations were generated.\")\n",
        "\n",
        "# Optional: Print the shape of X_repr to verify\n",
        "print(f\"Shape of generated X_repr: {X_repr.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifTosYsIg_zI",
        "outputId": "2454e0e1-7b90-446b-c6e8-c32e15b74eb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of generated X_repr: (125973, 64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Train/evaluate a classifier (LogisticRegression and RandomForest for comparison)\n",
        "\n",
        "# Check shapes before splitting\n",
        "print(f\"Shape of X_repr: {X_repr.shape}\")\n",
        "print(f\"Shape of y_all: {y_all.shape}\")\n",
        "\n",
        "if X_repr.shape[0] != y_all.shape[0]:\n",
        "    print(\"Error: Number of samples in X_repr and y_all do not match.\")\n",
        "    print(\"Please check the representation building step (cell 8) to ensure X_repr is generated correctly.\")\n",
        "elif len(np.unique(y_all)) < 2:\n",
        "     print(\"Error: The target variable y_all contains only one class. Cannot train a binary classifier.\")\n",
        "     print(\"Please check your data loading and preprocessing steps to ensure both classes are present in the original data.\")\n",
        "else:\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_repr, y_all, test_size=0.3, random_state=42, stratify=y_all)\n",
        "\n",
        "    # Check if there are at least two classes in y_train after splitting\n",
        "    if len(np.unique(y_train)) < 2:\n",
        "        print(\"Error: The training data (y_train) contains only one class after splitting. Cannot train a binary classifier.\")\n",
        "        print(\"Please check the stratification in the train_test_split or the original y_all.\")\n",
        "    else:\n",
        "        clf = LogisticRegression(max_iter=1000)\n",
        "        clf.fit(X_train, y_train)\n",
        "        y_pred = clf.predict(X_test)\n",
        "        print(\"Logistic Regression on contrastive reps\")\n",
        "        print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "        print(classification_report(y_test, y_pred, digits=4))\n",
        "        print(\"ROC AUC:\", roc_auc_score(y_test, clf.predict_proba(X_test)[:,1]))\n",
        "\n",
        "        # You can add the RandomForestClassifier evaluation here as well if needed\n",
        "        # clf_rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "        # clf_rf.fit(X_train, y_train)\n",
        "        # y_pred_rf = clf_rf.predict(X_test)\n",
        "        # print(\"\\nRandom Forest on contrastive reps\")\n",
        "        # print(\"Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
        "        # print(classification_report(y_test, y_pred_rf, digits=4))\n",
        "        # print(\"ROC AUC:\", roc_auc_score(y_test, clf_rf.predict_proba(X_test)[:,1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scPYyL82hzzP",
        "outputId": "d858b4f7-9086-4d3d-f8f9-8ee147281772"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_repr: (125973, 64)\n",
            "Shape of y_all: (125973,)\n",
            "Logistic Regression on contrastive reps\n",
            "Accuracy: 0.9739627434377646\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9704    0.9812    0.9758     20203\n",
            "           1     0.9782    0.9656    0.9718     17589\n",
            "\n",
            "    accuracy                         0.9740     37792\n",
            "   macro avg     0.9743    0.9734    0.9738     37792\n",
            "weighted avg     0.9740    0.9740    0.9740     37792\n",
            "\n",
            "ROC AUC: 0.9967891060097844\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3ZwHLL-Ih7sp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}